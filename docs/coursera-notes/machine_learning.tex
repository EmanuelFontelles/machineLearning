\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[]{article}
\usepackage[top=2.5cm,left=2.5cm,right=2.5cm,bottom=2.5cm]{geometry} %margens
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\usepackage{hyperref}
\hypersetup{
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\newenvironment{Shaded}{}{}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother


\date{}

\begin{document}

\hypertarget{machine-learning}{%
\section{Machine Learning}\label{machine-learning}}

\hypertarget{week-1}{%
\subsection{Week 1}\label{week-1}}

\hypertarget{introduction}{%
\subsubsection{Introduction}\label{introduction}}

\begin{itemize}
\tightlist
\item
  A computer program learn from experience E with respect to some task T
  and some performance measure P, if its performance on T as measured by
  P improves with experience E
\item
  Supervised learning

  \begin{itemize}
  \tightlist
  \item
    In the dataset we are told the right answer of each data
  \item
    Regression - predict value
  \item
    Classification
  \end{itemize}
\item
  Unsupervised learning

  \begin{itemize}
  \tightlist
  \item
    Given date with no labels
  \item
    Clustering
  \end{itemize}
\end{itemize}

\hypertarget{linear-regression-with-one-variable}{%
\subsubsection{Linear Regression with One
Variable}\label{linear-regression-with-one-variable}}

\hypertarget{model-representation}{%
\paragraph{Model Representation}\label{model-representation}}

\begin{itemize}
\tightlist
\item
  Training data ( (x1, y1), (x2, y2), \ldots{} )-\textgreater{} learning
  algorithm -\textgreater{} hypothesis
\item
  A linear regression:
\end{itemize}

\hypertarget{cost-function}{%
\paragraph{Cost Function}\label{cost-function}}

\begin{itemize}
\tightlist
\item
  thetas: parameters
\item
  Different parameters give different hypothesis
\item
  Come up with paramters that fit the data well - choose paramters so
  that h(x) is close to y for training example (x, y)
\item
  One cost function:
\end{itemize}

\begin{itemize}
\tightlist
\item
  Goal:
\end{itemize}

\begin{itemize}
\tightlist
\item
  Intuition: sum of distance between hypothesis and the actual values
\end{itemize}

\hypertarget{gradient-descent}{%
\paragraph{Gradient Descent}\label{gradient-descent}}

\begin{itemize}
\tightlist
\item
  Idea: start with some initial paramters; keep changing them to reduce
  cost
\item
  Algorithm: repeat until convergence
\end{itemize}

\begin{itemize}
\tightlist
\item
  Derivates: tangent of the point in the function J
\item
  Alpha: learning rate - how large the step it takes

  \begin{itemize}
  \tightlist
  \item
    Too small: the algorithm can be small
  \item
    Too large: overshoot the minimum, may never converge
  \item
    As we approach local minimum, gradient descent will automatically
    take smaller step; so no need to decrease alpha
  \end{itemize}
\item
  Simultaneously update parameters
\item
  Apply gradient descent to cost function
\item
  Batch gradient descent: use all training examples in each step
\end{itemize}

\hypertarget{week-2}{%
\subsection{Week 2}\label{week-2}}

\hypertarget{multiple-features-multivariate-linear-regression}{%
\subsubsection{Multiple Features (Multivariate Linear
Regression)}\label{multiple-features-multivariate-linear-regression}}

\begin{itemize}
\tightlist
\item
  Hypothesis:
\end{itemize}

~

\begin{itemize}
\tightlist
\item
  x0 is equal to 1
\end{itemize}

\hypertarget{gradient-descent-1}{%
\subsubsection{Gradient Descent}\label{gradient-descent-1}}

\begin{itemize}
\tightlist
\item
  Cost function:
\end{itemize}

\begin{itemize}
\tightlist
\item
  Algorithm:
\end{itemize}

\begin{itemize}
\tightlist
\item
  Simultaneously update all parameters
\item
  Have the same form as gradient descent with single variable
\end{itemize}

\hypertarget{feature-scaling}{%
\subsubsection{Feature Scaling}\label{feature-scaling}}

\begin{itemize}
\tightlist
\item
  Make sure features are on a similar scale
\item
  It will take a long time for gradient descent to converge if they are
  not
\item
  Get every feature into approximately {[}-1, 1{]}
\item
  Can also use mean normalization

  \begin{itemize}
  \tightlist
  \item
    replace xi with (xi - mean) / {[}(max - min) \textbar{} stdev{]}
  \end{itemize}
\end{itemize}

\hypertarget{learning-rate}{%
\subsubsection{Learning Rate}\label{learning-rate}}

\begin{itemize}
\tightlist
\item
  Making sure that gradient descent work correctly

  \begin{itemize}
  \tightlist
  \item
    Plot cost function versus number of iterations
  \item
    The value should decrease after each iteration
  \item
    Use automatic convergence test: declare convergence after the
    decrease is smaller than a threshold
  \end{itemize}
\item
  If it is not working correctly

  \begin{itemize}
  \tightlist
  \item
    Use small learning rate
  \item
    But a small learning rate will make convergence slow
  \end{itemize}
\item
  In summary

  \begin{itemize}
  \tightlist
  \item
    If learning rate is small, slow convergence
  \item
    If learning rate is large, may not converge
  \item
    Try a range of learning rate in practice
  \end{itemize}
\end{itemize}

\hypertarget{features-and-polynominal-regression}{%
\subsubsection{Features and Polynominal
Regression}\label{features-and-polynominal-regression}}

\begin{itemize}
\tightlist
\item
  Sometimes choosing different features yield a better model
\item
  Polynominal regression

  \begin{itemize}
  \tightlist
  \item
    For example, suppose we want to fit a model:
  \end{itemize}
\end{itemize}

\begin{itemize}
\tightlist
\item
  We can turn it to a linear regression by having:
\end{itemize}

\begin{itemize}
\tightlist
\item
  Features scaling becomes very important
\item
  By choosing different features you can get different model
\end{itemize}

\hypertarget{normal-equation}{%
\subsubsection{Normal Equation}\label{normal-equation}}

\begin{itemize}
\tightlist
\item
  Method for solving theta analytically
\item
  Intuition:
\end{itemize}

\begin{itemize}
\tightlist
\item
  Optimal Solution
\end{itemize}

\begin{itemize}
\tightlist
\item
  No need to do feature scaling
\item
  Comparison with gradient descent

  \begin{itemize}
  \tightlist
  \item
    Gradient descent

    \begin{itemize}
    \tightlist
    \item
      Need to choose learning rate
    \item
      Need many iterations
    \item
      Works well even when n (number of features) is large
    \end{itemize}
  \item
    Normal equation

    \begin{itemize}
    \tightlist
    \item
      No need to choose learning rate
    \item
      Don't need iterations
    \item
      Need to compute (X'X)\^{}-1 - O(n\^{}3)
    \item
      Slow if n is large
    \item
      Doesn't work for some problems
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{normal-equation-and-noninvertibility}{%
\subsubsection{Normal Equation and
Noninvertibility}\label{normal-equation-and-noninvertibility}}

\begin{itemize}
\tightlist
\item
  What if X'X is non-invertible

  \begin{itemize}
  \tightlist
  \item
    May have redundant features
  \item
    Too many features

    \begin{itemize}
    \tightlist
    \item
      Delete some features or use regularization
    \end{itemize}
  \end{itemize}
\item
  pinv (pseudo inverse) in Octave will calculate correctly even X is not
  invertible
\end{itemize}

\hypertarget{week-3}{%
\subsection{Week 3}\label{week-3}}

\hypertarget{logistic-regression}{%
\subsubsection{Logistic Regression}\label{logistic-regression}}

\begin{itemize}
\tightlist
\item
  Classification problmes

  \begin{itemize}
  \tightlist
  \item
    Classify into ``0'' (negative class) or ``1'' (positive class)
  \item
    Or multi class problem (classify into ``0'', ``1'', ``2'', \ldots{})
  \end{itemize}
\item
  Can use linear regression

  \begin{itemize}
  \tightlist
  \item
    Threshold classifier: \textgreater{} 0.5, predict ``1'', otherwise
    predict ``0''
  \item
    Not a good idea

    \begin{itemize}
    \tightlist
    \item
      Often it is right beacause you are lucky (e.g., specific training
      data)
    \item
      Output value can be \textgreater{} 1 or \textless{} 0 while we
      want output to be 0 or 1
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{hypothesis-representation}{%
\subsubsection{Hypothesis
Representation}\label{hypothesis-representation}}

\begin{itemize}
\tightlist
\item
  Want prediction to be between 0 and 1
\item
  Use logistic (sigmoid) function
\end{itemize}

\begin{itemize}
\tightlist
\item
  Output can be interpreted as the probability that y = 1 given x,
  parameterized by theta
\end{itemize}

\hypertarget{desicion-boundary}{%
\subsubsection{Desicion Boundary}\label{desicion-boundary}}

\begin{itemize}
\tightlist
\item
  Predict ``1'' if output \textgreater{}= 0.5, else predict ``0''
\item
  If the underlying regression outputs \textgreater{}= 0, the output
  will be \textgreater{}= 0.5
\item
  Fit a line that separated the region where the hypothesis predicts 1
  and that predicts 0

  \begin{itemize}
  \tightlist
  \item
    The line is desicion booundary
  \end{itemize}
\item
  Nonlinear decision boundaries

  \begin{itemize}
  \tightlist
  \item
    Fit a polynominal desicion boundary
  \end{itemize}
\end{itemize}

\hypertarget{cost-function-1}{%
\subsubsection{Cost Function}\label{cost-function-1}}

\begin{itemize}
\tightlist
\item
  Take the cost function from linear regression
\end{itemize}

\begin{itemize}
\tightlist
\item
  This will give us a non-convex function (lots of local optima),
  because hypothesis uses logistic function
\item
  Cost function for logistic regression
\end{itemize}

\begin{itemize}
\tightlist
\item
  Capture the intuition that if y = h, cost is 0. If h = 0 and y = 1,
  the cost is infinity and vice versa.
\end{itemize}

\hypertarget{simplified-cost-function-and-gradient-descent}{%
\subsubsection{Simplified Cost Function and Gradient
Descent}\label{simplified-cost-function-and-gradient-descent}}

\begin{itemize}
\tightlist
\item
  A simpler cost function
\end{itemize}

\begin{itemize}
\item
  Can be derived from statistics using the principle of maximum
  liklihood
\item
  Convex function
\item
  To fit the parameters, minimize the cost function to get a set of
  parameters

  \begin{itemize}
  \tightlist
  \item
    Use gradient descent, the algorithm looks idential to linear
    regression
  \item
    Feature scaling also applies to logistic regression
  \end{itemize}
\end{itemize}

\hypertarget{advanced-optimization}{%
\subsubsection{Advanced Optimization}\label{advanced-optimization}}

\begin{itemize}
\tightlist
\item
  Optimization algorithms

  \begin{itemize}
  \tightlist
  \item
    Conjugate gradient
  \item
    BFGS
  \item
    L-BFGS
  \end{itemize}
\item
  These algorithms

  \begin{itemize}
  \tightlist
  \item
    No need to pick up a learning rate
  \item
    Often much faster to converge
  \item
    More complex
  \end{itemize}
\end{itemize}

\hypertarget{multiclass-classification-one-vs-all}{%
\subsubsection{Multiclass Classification:
One-vs-all}\label{multiclass-classification-one-vs-all}}

\begin{itemize}
\tightlist
\item
  One-vs-all

  \begin{itemize}
  \tightlist
  \item
    Suppose we have three classes
  \item
    Turn the training data into three separate binary classification
    problems
  \item
    Train three classifiers
  \item
    Pick the class i that maximizes h
  \end{itemize}
\end{itemize}

\hypertarget{the-problem-of-overfitting}{%
\subsubsection{The Problem of
Overfitting}\label{the-problem-of-overfitting}}

\begin{itemize}
\tightlist
\item
  Underfit (high bias)

  \begin{itemize}
  \tightlist
  \item
    Algorithm not fitting the data well
  \end{itemize}
\item
  Overfit (high variance)

  \begin{itemize}
  \tightlist
  \item
    If we have too many features, the learned hypothesis may fit the
    training set very well, but fail to generalize to new examples
  \end{itemize}
\item
  Addressing overfitting

  \begin{itemize}
  \tightlist
  \item
    Reduce number of features

    \begin{itemize}
    \tightlist
    \item
      Manually select features
    \item
      Model selection to automatically select features
    \item
      It also throws away useful information\\
    \end{itemize}
  \item
    Regularization

    \begin{itemize}
    \tightlist
    \item
      Keep all features, but reduce magnitude/values of parameter
    \item
      Works well if we have lots of features and each of them
      contributes a bit
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{cost-function-2}{%
\subsubsection{Cost Function}\label{cost-function-2}}

\begin{itemize}
\tightlist
\item
  If we have small values of parameters

  \begin{itemize}
  \tightlist
  \item
    We have a simpler hypothesis
  \item
    Less prone to overfitting
  \end{itemize}
\item
  Take cost function and shrink all parameters
\end{itemize}

\begin{itemize}
\tightlist
\item
  By convention, not penalizing theta0
\item
  Lambda is a regularization parameter

  \begin{itemize}
  \tightlist
  \item
    Control the tradeoff between fitting the training set well and
    keeping parameters small
  \item
    Too large lambda will penalize parameters too much, thus causing
    underfitting
  \end{itemize}
\end{itemize}

\hypertarget{regularized-linear-regression}{%
\subsubsection{Regularized Linear
Regression}\label{regularized-linear-regression}}

\begin{itemize}
\tightlist
\item
  Gradient descent

  \begin{itemize}
  \tightlist
  \item
    Regualarization term shrinks theta a little for each iteration
  \item
    For j = 0, the update rule doesn't change
  \item
    For j = 1, 2, 3, \ldots{}
  \end{itemize}
\end{itemize}

\begin{itemize}
\tightlist
\item
  Normal Equation

  \begin{itemize}
  \tightlist
  \item
    Non-invertibility

    \begin{itemize}
    \tightlist
    \item
      Suppose number of training data \textless{}= number of features.
      If lambda \textgreater{} 0, the matrix will be invertable
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{regularized-logistic-regression}{%
\subsubsection{Regularized Logistic
Regression}\label{regularized-logistic-regression}}

\begin{itemize}
\tightlist
\item
  Add the regularization term to the cost function

  \begin{itemize}
  \tightlist
  \item
    Same update rule for linear regression
  \end{itemize}
\end{itemize}

\hypertarget{week-4-neural-networks-representation}{%
\subsection{Week 4: Neural Networks:
Representation}\label{week-4-neural-networks-representation}}

\hypertarget{non-linear-hypothesis}{%
\subsubsection{Non-linear Hypothesis}\label{non-linear-hypothesis}}

\begin{itemize}
\tightlist
\item
  Many features

  \begin{itemize}
  \tightlist
  \item
    if include quadratic features, there are too many features and maybe
    overfitting
  \item
    if not, not enough features to fit the data set
  \end{itemize}
\item
  Simple logistic regression with quadratic features added in is not a
  good way to learn complex hypothesis
\end{itemize}

\hypertarget{neurons-and-brain}{%
\subsubsection{Neurons and Brain}\label{neurons-and-brain}}

\begin{itemize}
\tightlist
\item
  Origin: mimic brain
\item
  Popularity: 80s, early 90s, diminished later
\item
  Resurgence: more powerful computers
\end{itemize}

\hypertarget{model-representation-1}{%
\subsubsection{Model Representation}\label{model-representation-1}}

\begin{itemize}
\tightlist
\item
  Neuron unit: logistic unit

  \begin{itemize}
  \tightlist
  \item
    Feed in some inputs, the neuron does some computation and outputs
  \item
    A sigmoid(logistic) activation function
  \end{itemize}
\item
  Network: neuron unit wired together

  \begin{itemize}
  \tightlist
  \item
    Layer 1: input layer
  \item
    Layer 2: hidden layer, can have many
  \item
    Layer 3: output layer
  \end{itemize}
\end{itemize}

\begin{itemize}
\tightlist
\item
  Forward propagation
\end{itemize}

\begin{itemize}
\tightlist
\item
  Neural networks learn its own features

  \begin{itemize}
  \tightlist
  \item
    Like logistic regression
  \item
    Use the computed features \emph{a} insteand of the original feature
    \emph{x}
  \item
    Hidden layer computed more complex features
  \end{itemize}
\item
  Multi-class Classification

  \begin{itemize}
  \tightlist
  \item
    Suppose we have four classes
  \item
    Have four output units
  \item
    Want {[}1 0 0 0{]} for class 1, {[}0 1 0 0{]} for class 2, and so on
  \item
    For training set, represent as {[}1 0 0 0{]} and so on
  \end{itemize}
\end{itemize}

\hypertarget{week-5---neural-networks-learning}{%
\subsection{Week 5 - Neural Networks:
Learning}\label{week-5---neural-networks-learning}}

\hypertarget{cost-function-3}{%
\subsubsection{Cost Function}\label{cost-function-3}}

\hypertarget{backpropagation-algorithm}{%
\subsubsection{Backpropagation
Algorithm}\label{backpropagation-algorithm}}

\begin{itemize}
\tightlist
\item
  Want to minimize cost function
\item
  Need to compute cost function and gradient
\item
  Intuition: calculate error of node j in layer l
\end{itemize}

\begin{itemize}
\tightlist
\item
  Backpropagation algorithm
\end{itemize}

\begin{verbatim}
For traning examples 1 to m
    Set a(1) = x(i)
    Perform forward propagation to compute a(l) for l = 2 to L
    Using y(i) to compute delta(L) = a(L) - y(i)
    Compute delta(L - 1) to delta(2)
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Formally delta is partial derivative of cost over \emph{z}
\item
  delta of \emph{(i - 1)} is calculated by deltas from \emph{i} weighted
  by the parameters
\end{itemize}

\hypertarget{gradient-checking}{%
\subsubsection{Gradient Checking}\label{gradient-checking}}

\begin{itemize}
\tightlist
\item
  Calculate a approximate value of the derivative and compare with
  gradient
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{for }\FloatTok{1}\NormalTok{ = }\FloatTok{1}\NormalTok{:n}
\NormalTok{    thetaPlus = theta;}
\NormalTok{    thetaPlus(i) = thetaPlus(i) + EPSILON;}
\NormalTok{    thetaMinus = theta;}
\NormalTok{    thetaMinus(i) = thetaPlus(i) - EPSILON;}
\NormalTok{    gradApprox(i) = (J(thetaPlus) - J(thetaMinus)) / (}\FloatTok{2}\NormalTok{ * EPSILON);}
\NormalTok{end;}
\CommentTok{% Compare with gradient}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Turn off gradient checking once the code is verified to be correct
\end{itemize}

\hypertarget{random-initialization}{%
\subsubsection{Random Initialization}\label{random-initialization}}

\begin{itemize}
\tightlist
\item
  Initial value of 0 does not work for neural networks

  \begin{itemize}
  \tightlist
  \item
    All activiation ouputs are the same, the errors will also be same
  \item
    Gradient will be the same
  \item
    After each update, parameters corresponding to inputs going into
    each group of two hidden units are identical
  \end{itemize}
\item
  Initial to be value between -epsilon and epsilon
\end{itemize}

\hypertarget{putting-it-together}{%
\subsubsection{Putting it together}\label{putting-it-together}}

\begin{itemize}
\tightlist
\item
  Pick a network architecture

  \begin{itemize}
  \tightlist
  \item
    Number of input units: dimension of input \emph{x}
  \item
    Number of output units: number of classes
  \item
    Reasonable default: 1 hidden layer, or if \textgreater{}1 hiddent
    layer, use same number of hidden units in each layer (usually the
    more the better)
  \end{itemize}
\item
  Training network

  \begin{itemize}
  \tightlist
  \item
    Random initialization
  \item
    Implement forward propagation
  \item
    Implement cost function
  \item
    Implement back propagation to get derivatives
  \item
    Use gradient checking, then disable it
  \item
    Use gradient descent or other advanced algorithm to minimize cost
    function

    \begin{itemize}
    \tightlist
    \item
      Cost function is non-convext, so will probably stuck in local
      minimium
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{week-7-support-vector-machine}{%
\subsection{Week 7: Support Vector
Machine}\label{week-7-support-vector-machine}}

\hypertarget{optimization-goals}{%
\subsubsection{Optimization Goals}\label{optimization-goals}}

\begin{itemize}
\tightlist
\item
  Cost function for SVM
\end{itemize}

\begin{itemize}
\tightlist
\item
  C is 1 / lambda
\item
  For cost 1, cost is 0 if z \textgreater{}= 1; for cost 0, cost is 0 if
  z \textless{}= -1
\item
  Output 1 or 0, not probability
\end{itemize}

\hypertarget{large-margin-intuition}{%
\subsubsection{Large Margin Intuition}\label{large-margin-intuition}}

\begin{itemize}
\tightlist
\item
  If y = 1, we want z \textgreater{}= 1
\item
  If y = 0, we want z \textless{}= -1
\item
  Suppose C is very large, then we want the first term in cost function
  to be small

  \begin{itemize}
  \tightlist
  \item
    Choose theta such that when y = 1, z \textgreater{}= 1, z
    \textless{}= -1 when y = 0
  \item
    Solve this, we will get a decision boudary that separates datasets
    at the maximum margin
  \item
    This is sensitive to outliers
  \item
    If we make C small, it is less sensitive to outliers
  \end{itemize}
\end{itemize}

\hypertarget{kernels}{%
\subsubsection{Kernels}\label{kernels}}

\begin{itemize}
\tightlist
\item
  We have the formula theta0 + theta1 * f1 + theta2 * f2 + \ldots{}
\item
  Predict 1 if the above formula \textgreater{}= 0
\item
  We choose some landmarks and have fi = similarity(x, li)

  \begin{itemize}
  \tightlist
  \item
    Similarity function is the kernel
  \item
    For example, we can use a gaussian kernel:
    exp(-\textbar{}\textbar{}xi - li\textbar{}\textbar{}\^{}2 / (2 *
    sigma\^{}2))

    \begin{itemize}
    \tightlist
    \item
      sigma controls the ``width''
    \end{itemize}
  \item
    If xi is close to li, fi is close to 1
  \item
    If xi is far away from li, fi is close to 0
  \end{itemize}
\item
  Choosing the landmarks

  \begin{itemize}
  \tightlist
  \item
    For every training example, choose a landmark at the exact same
    location
  \end{itemize}
\item
  For every training example

  \begin{itemize}
  \tightlist
  \item
    Have f1 = sim(xi, l1), f2 = sim(xi, l2), f3 = sim(xi, l3), \ldots{}
  \item
    Put these f into a vector, f0 = 1
  \item
    Predict 1 if theta' * f \textgreater{}= 0
  \end{itemize}
\item
  Use some computational tricks

  \begin{itemize}
  \tightlist
  \item
    Kernel is not used for other methods because the trick doesn't work
    well
  \end{itemize}
\item
  Large C, high variance
\item
  Small C, low variance
\item
  Large sigma, features fi vary more smoothly. Higher bias, lower
  variance
\item
  Small sigma, features fi vary less smoothly. Lower bias, higher
  variance
\end{itemize}

\hypertarget{using-svm}{%
\subsubsection{Using SVM}\label{using-svm}}

\begin{itemize}
\tightlist
\item
  Use software library
\item
  Have to choose

  \begin{itemize}
  \tightlist
  \item
    Parameters
  \item
    Choice of kernel

    \begin{itemize}
    \tightlist
    \item
      No kernel: theta0 + theta1 * x1 + \ldots{} + thetan * xn
      \textgreater{}= 0 If n is large and m is small, you can use no
      kernel
    \item
      Gaussian kernel

      \begin{itemize}
      \tightlist
      \item
        Need to choose sigma
      \item
        If n is small and m is large
      \item
        Do perform feature scaling
      \end{itemize}
    \item
      Not all similarity functions are valid kernel

      \begin{itemize}
      \tightlist
      \item
        Must satisfy Mercer's Theorem
      \end{itemize}
    \end{itemize}
  \end{itemize}
\item
  Multi-class SVM

  \begin{itemize}
  \tightlist
  \item
    Use built-in functions in packages
  \item
    Use one-vs-all
  \end{itemize}
\item
  Logistic regression vs SVM

  \begin{itemize}
  \tightlist
  \item
    If n is large, use logistic regression or SVM with no kernel
  \item
    If n is small, m is intermediate, use SVM with Gaussian kernel
  \item
    If n is small, m is large, create/add more features, use logistic
    regression or SVM with no kernel
  \item
    Neural network tends to work well in all these settings, but is slow
    to train
  \end{itemize}
\end{itemize}

\hypertarget{week-8-clustering}{%
\subsection{Week 8: Clustering}\label{week-8-clustering}}

\hypertarget{k-means-algorithm}{%
\subsubsection{K-means Algorithm}\label{k-means-algorithm}}

\hypertarget{algorithm}{%
\paragraph{Algorithm}\label{algorithm}}

\begin{itemize}
\tightlist
\item
  Input: K(number of clusters), training set \{x1, x2, \ldots{}\}
\item
  Randomly initialize K cluster centroids
\item
  Repeat

  \begin{itemize}
  \tightlist
  \item
    For every training example, assign it to the closest centroid
  \item
    For every cluster, compute a new centroid based on the mean of
    points assigned to the cluster
  \item
    Eliminate cluster with no points to it \#\#\#\# Optimization
    objective
  \end{itemize}
\end{itemize}

\begin{itemize}
\tightlist
\item
  Find the set of parameters that minimize this cost function
\item
  The first part of the algorithm minimize it with respect to c
\item
  The second part of the algorithm minimize it with respect to centroid
  \#\#\#\# Random Initialization
\item
  Should have K \textless{} m
\item
  Randomly pick K training examples
\item
  Set centroids to be these examples
\item
  Depending on different initialization, K-mens can be at local optima

  \begin{itemize}
  \tightlist
  \item
    Try multiple initialization
  \item
    Run K-means
  \item
    Compute cost functions
  \item
    Pick one with lowest cost
  \item
    Works well for K between 2 to 10
  \item
    If K is large, generally the first initialization will give a good
    result \#\#\#\# Choosing the number of clusters
  \end{itemize}
\item
  Elbow method

  \begin{itemize}
  \tightlist
  \item
    Vary K
  \item
    Plot K vs cost J
  \item
    Find a elbow that before the cost reduces rapidly but slowly after
  \item
    But many times there is no clear elbow
  \end{itemize}
\item
  Evaluate K-means based on a metric for how well it performs for later
  purpose
\end{itemize}

\hypertarget{week-8-dimensionality-reduction}{%
\subsection{Week 8: Dimensionality
Reduction}\label{week-8-dimensionality-reduction}}

\hypertarget{motivation}{%
\subsubsection{Motivation}\label{motivation}}

\begin{itemize}
\tightlist
\item
  Data compression

  \begin{itemize}
  \tightlist
  \item
    Reduce 2D to 1D, 3D to 2D, etc.
  \item
    Some features may be redundant or highly related
  \item
    Save memory and space
  \item
    Make learning algorithm faster
  \end{itemize}
\item
  Visualization \#\#\# Principal Component Analysis
\item
  Problem Formulation

  \begin{itemize}
  \tightlist
  \item
    Try to find k vectors onto which to project data so that the
    projection error is minimized
  \end{itemize}
\item
  Algorithm

  \begin{itemize}
  \tightlist
  \item
    Preprocessing

    \begin{itemize}
    \tightlist
    \item
      Compute mean of each feature
    \item
      Replace each training example with xj - meanj
    \item
      If different features are on different scale, scale features to
      have comparable range of values
    \end{itemize}
  \item
    Suppose we want reduce from n-dimensions to k-dimensions

    \begin{itemize}
    \tightlist
    \item
      Compute a covariance matrix
    \item
      Compute eigenvectors of covariance matrix
    \item
      U is the vectors we want: each column is the vector ui
    \item
      Use the first k column as u1, \ldots{} uk
    \item
      x = Ureduce' * x
    \item
      No x0 = 1 convention
    \end{itemize}
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{[U, S, V] = svd(Sigma);}
\NormalTok{Ureduce = U(:, }\FloatTok{1}\NormalTok{:k);}
\NormalTok{z = Ureduce' * x;}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Choosing number of principal components

  \begin{itemize}
  \tightlist
  \item
    Average squared projection error: 1/m * sum(\textbar{}\textbar{}x(i)
    - xapprox(i)\textbar{}\textbar{}\^{}2)
  \item
    Total variation in the data: 1/m *
    sum(\textbar{}\textbar{}x(i)\textbar{}\textbar{}\^{}2)
  \item
    Their ratio(average error / variation) is \textless{}= 0.01
  \item
    99\% variance is retained
  \item
    Algorithm

    \begin{itemize}
    \tightlist
    \item
      Try k = 1
    \item
      If ratio is \textless{}= 0.01, if so use this k, else try next k
    \item
      Alternatively, use the S matrix from svd

      \begin{itemize}
      \tightlist
      \item
        For given k, ratio can be computed 1 - (sum(Sii) from 1 to k) /
        (sum(Sii) from 1 to n)
      \item
        Just need to run svd once and try different k until the ratio is
        small enough
      \end{itemize}
    \end{itemize}
  \end{itemize}
\item
  Reconstruction from Compressed Representation

  \begin{itemize}
  \tightlist
  \item
    xapprox = Ureduce * z, roughly equal to x
  \end{itemize}
\item
  Use PCA to speedup some learning algorithms

  \begin{itemize}
  \tightlist
  \item
    In supervised learning, the dimension of feature may be very large

    \begin{itemize}
    \tightlist
    \item
      Apply PCA to the training set to get a new training set
    \item
      Map test data to the same dimension and make a prediction
    \end{itemize}
  \end{itemize}
\item
  Bad use of PCA: to prevent overfitting

  \begin{itemize}
  \tightlist
  \item
    Regularization will work as well if you want to retain 99\% of
    variance
  \item
    Will throw away useful information
  \end{itemize}
\item
  Generally first try to run the whole thing with raw data, and use PCA
  only if this doesn't work well
\end{itemize}

\hypertarget{week-9-anomaly-detection}{%
\subsection{Week 9: Anomaly Detection}\label{week-9-anomaly-detection}}

\hypertarget{motivation-1}{%
\subsubsection{Motivation}\label{motivation-1}}

\begin{itemize}
\tightlist
\item
  Have a training dataset and xtest
\item
  Have a model p(x)
\item
  If p(xtest) \textless{} epsilon, flag as anomaly, else ok
\item
  Example uses

  \begin{itemize}
  \tightlist
  \item
    Fraud detection
  \item
    Manufacturing
  \item
    Monitoring computer cluster and data center \#\#\# Algorithm
  \end{itemize}
\item
  Suppose we have training set with size m
\item
  Each example has n features
\item
  Choose features xi that you think might be indicators for anomaly
\item
  Compute mean and variance
\item
  p(x) = p(x1; mu1, sigma1)p(x2; mu2, sigma2)\ldots{}p(xn; mu(n).
  sigma(n))

  \begin{itemize}
  \tightlist
  \item
    Assume each feature has a Gaussian distribution
  \end{itemize}
\item
  Anomaly if p(x) \textless{} epsilon \#\#\# Developing and Evaluating
  Anomaly Detection System
\item
  Suppose we have some labeled data of anomalous and non-anomalous
  examples
\item
  We have training examples, usually normal
\item
  We have cross validation set and test set that can include anomalous
  examples
\item
  On a cross-validation or test example, predict y
\item
  Possible metrics: true positive false positive, false negative, true
  negative, precision/recall, F-1 score
\item
  Can also use cross validation set to set the value of epsilon \#\#\#
  Vs Supervised Learning
\item
  You should use anomaly detection

  \begin{itemize}
  \tightlist
  \item
    If you have very small number of positive examples and large number
    of negative examples,
  \item
    Many different types of anomalies; hard for algorithm to learn
  \item
    Future anomaly may not like any seen so far
  \end{itemize}
\item
  You should use supervised learning

  \begin{itemize}
  \tightlist
  \item
    If you have large number of positive and negative examples
  \item
    Enough positive examples to learn
  \item
    Future positive examples may be similar to the ones already seen
    \#\#\# Choosing Features
  \end{itemize}
\item
  For non-gaussian features

  \begin{itemize}
  \tightlist
  \item
    Take a log of data
  \item
    Transform data into gaussian-like data
  \end{itemize}
\item
  Error analysis for anomaly detection

  \begin{itemize}
  \tightlist
  \item
    Want p(x) large for normal examples but small for anomaly
  \item
    Most common problem: p(x) is comparable for both normal and
    anomalous examples

    \begin{itemize}
    \tightlist
    \item
      Look at the failing example, create some new features to capture
      that
    \end{itemize}
  \end{itemize}
\item
  Choose features that may take on unusually large or small values in
  the case of anomaly \#\#\# Multivariate Gaussian Distribution
\item
  Model all variables altogether, not separately
\item
  Parameters: mean and covariance matrix
\item
  Can be model corelation between data
\end{itemize}

\begin{itemize}
\tightlist
\item
  Parameter fitting:
\end{itemize}

\begin{itemize}
\tightlist
\item
  Algorithm:

  \begin{itemize}
  \tightlist
  \item
    Fit model
  \item
    Given a test example, compute p(x), flag anomaly if p(x) is small
  \end{itemize}
\item
  Relationship to original model:

  \begin{itemize}
  \tightlist
  \item
    Axies aligned with X and Y
  \item
    Covariance matrix must have 0 on off diagnal
  \end{itemize}
\item
  Use cases

  \begin{itemize}
  \tightlist
  \item
    Original model

    \begin{itemize}
    \tightlist
    \item
      Manually create features that capture anomaly
    \item
      Computationaly cheaper, scale better to larger n
    \item
      Ok if small training examples
    \end{itemize}
  \item
    Multivariate Gaussian

    \begin{itemize}
    \tightlist
    \item
      Automatically capture correlations between different features
    \item
      Computationaly expensive
    \item
      Must have m \textgreater{} n, or covariance matrix is not
      invertible
    \item
      If you have redudant features, covariance matrix is not invertible
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{week-9-recommender-system}{%
\subsection{Week 9: Recommender
System}\label{week-9-recommender-system}}

\hypertarget{content-based-recommendation}{%
\subsubsection{Content Based
Recommendation}\label{content-based-recommendation}}

\begin{itemize}
\tightlist
\item
  For each user perform a linear regression to learn a set of parameters
\item
  Have features for each content
\item
  Use the parameters and features to predict
\item
  Learn parameters for all users; use gradient descent
\item
  Content features are not always available
\end{itemize}

\hypertarget{collaborative-filtering}{%
\subsubsection{Collaborative Filtering}\label{collaborative-filtering}}

\begin{itemize}
\tightlist
\item
  Given parameters, to learn content feature vector
\item
  Find the content feature vector such that the prediction is not far
  from the true value
\end{itemize}

\begin{itemize}
\tightlist
\item
  Can guess parameters and learn content features
\item
  Then use content features to learn parameters
\item
  Repeat
\item
  Algorithm

  \begin{itemize}
  \tightlist
  \item
    Minimize x(1), x(2), \ldots{} and theta simultaneously
  \item
    Initialize x and theta to small random values
  \item
    Minimize the cost function
  \item
    For a user with parameters theta and learned content features, make
    a prediction
  \end{itemize}
\end{itemize}

\hypertarget{vectorization}{%
\subsubsection{Vectorization}\label{vectorization}}

\begin{itemize}
\tightlist
\item
  Low rank matrix factorization

  \begin{itemize}
  \tightlist
  \item
    Have a matrix X where each row is a content features vector
  \item
    Have a matrix Theta where each row is a parameter vector for each
    user
  \item
    X * Theta' gives the result
  \end{itemize}
\item
  Find related movies

  \begin{itemize}
  \tightlist
  \item
    For each product, we learn a feature vector
  \item
    Find movies such that the distance between their feature vectors is
    small \#\#\# Implementation Detail: Mean Normalization
  \end{itemize}
\item
  For a user who has not rated any movie, the parameters learned will be
  all 0
\item
  Subtract each rating for a movie by the average rating for that
\item
  Use the new ratings to learn parameters and features
\item
  For user, make a prediction using the learned parameters and features,
  then add the mean
\end{itemize}

\end{document}
