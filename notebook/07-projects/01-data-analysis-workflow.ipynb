{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import datetime;\n",
    "import csv;\n",
    "import logging;\n",
    "import os;\n",
    "import datetime;\n",
    "\n",
    "import numpy as np;\n",
    "import pandas as pd;\n",
    "\n",
    "from time import time, sleep;\n",
    "\n",
    "from sklearn import model_selection;\n",
    "from sklearn import metrics;\n",
    "from sklearn.metrics import make_scorer;\n",
    "from sklearn import preprocessing;\n",
    "\n",
    "from sklearn import datasets;\n",
    "\n",
    "from sklearn.dummy import DummyClassifier;\n",
    "\n",
    "from sklearn.decomposition import KernelPCA;\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif;\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV;\n",
    "\n",
    "from sklearn import linear_model;\n",
    "\n",
    "from sklearn.svm import SVC;\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier;\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin;\n",
    "from sklearn.pipeline import make_pipeline, FeatureUnion;\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer;\n",
    "\n",
    "import matplotlib;\n",
    "import matplotlib.pyplot as plt;\n",
    "import matplotlib.style as style;\n",
    "# Check for style.available for more styles;\n",
    "\n",
    "import seaborn as sns;\n",
    "\n",
    "# matplotlib.rcParams['font.family'] = 'sans-serif';\n",
    "# matplotlib.rcParams['font.sans-serif'] = ['Verdana'];\n",
    "\n",
    "# matplotlib.rcParams['font.family'] = 'cursive';\n",
    "#\n",
    "# matplotlib.rcParams['font.weight'] = 8;\n",
    "# matplotlib.rcParams['font.size'] = 9.5;\n",
    "\n",
    "matplotlib.rcParams['font.family'] = 'fantasy';\n",
    "\n",
    "matplotlib.rcParams['font.weight'] = 3;\n",
    "matplotlib.rcParams['font.size'] = 10;\n",
    "\n",
    "style.use('bmh');\n",
    "# style.use('seaborn-paper');\n",
    "# style.use('seaborn-deep');\n",
    "\n",
    "from itertools import chain;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2pandas_df(x_array=None, y=None, feature_names=None, target_name=None):\n",
    "    ''' list of datasets part of the sklearn ''';\n",
    "\n",
    "    assert x_array.shape[1] == len(feature_names);  # assert the length of x_array and column label length are same;\n",
    "    assert x_array.shape[0] == len(y); # The target length should equal the features length;\n",
    "    assert isinstance(y, list); # Target should of the type list;\n",
    "    assert isinstance(feature_names, list); # feature_names should of the type list;\n",
    "\n",
    "    data_dict = {};\n",
    "    data_dict[target_name] = y;\n",
    "\n",
    "    for i, col_name in enumerate(feature_names):\n",
    "        data_dict[col_name] = list(chain.from_iterable( x_array[:, [i]] ));\n",
    "\n",
    "    return pd.DataFrame(data_dict);\n",
    "\n",
    "\n",
    "def delete_old_log_files(delete_flag=False, logger=None, extension_list=None):\n",
    "    ''' Function to delete the old log files; cleanup process ''';\n",
    "\n",
    "    directory = './';\n",
    "    file_list = os.listdir(directory);\n",
    "\n",
    "    if delete_flag :\n",
    "        logger.info('DELETE_FLAG is set to true');\n",
    "        logger.info('All previous logfiles will be deleted');\n",
    "\n",
    "        logger.info(f'');\n",
    "        logger.info(f'{\"-\"*20} File deletion starts here {\"-\"*20}');\n",
    "        logger.info(f'');\n",
    "\n",
    "        fileName = \".\".join(__file__.split(\".\")[:-1]);\n",
    "\n",
    "        for item in file_list:\n",
    "            ext_flag = [ item.endswith(i) for i in extension_list ];\n",
    "            # logger.info(f'{ext_flag} | {item} | {np.sum(ext_flag)} | {fileName in item}');\n",
    "            if np.sum(ext_flag) and (fileName in item) and (LOG_TS not in item):\n",
    "                    os.remove(os.path.join(directory, item));\n",
    "                    logger.info(f'Deleted file : {item}');\n",
    "\n",
    "        logger.info(f'');\n",
    "        logger.info(f'{\"-\"*20} File deletion ends here {\"-\"*20}');\n",
    "        logger.info(f'');\n",
    "\n",
    "\n",
    "def chart_save_image(plt=None, f_size=None, left=None, right=None, bottom=None, top=None, wspace=None, hspace=None, fileName=None):\n",
    "    ''' Save the chart image with the set of specific options ''';\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(8, 4.5) # To maintain the 16:9 aspect ratio;\n",
    "\n",
    "    if f_size :\n",
    "        fig.set_size_inches(f_size[0], f_size[1]);\n",
    "\n",
    "    # https://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.subplots_adjust\n",
    "\n",
    "    # left          = 0.125     # the left side of the subplots of the figure\n",
    "    # right         = 0.9       # the right side of the subplots of the figure\n",
    "    # bottom        = 0.125     # the bottom of the subplots of the figure\n",
    "    # top           = 0.9       # the top of the subplots of the figure\n",
    "    # wspace        = 0.0       # the amount of width reserved for blank space between subplots,\n",
    "    #                           # expressed as a fraction of the average axis width\n",
    "    # hspace        = 0.0       # the amount of height reserved for white space between subplots,\n",
    "    #                           # expressed as a fraction of the average axis height\n",
    "\n",
    "    plt.subplots_adjust(left=left, bottom=bottom, right=right, top=top, wspace=wspace, hspace=hspace);\n",
    "    plt.savefig(f'{fileName}');\n",
    "    plt.clf();\n",
    "\n",
    "    # plt.savefig(f'./Make_Moons_Image.png', bbox_inches='tight', pad_inches = 1);\n",
    "\n",
    "def cost_accuracy(actual, prediction):\n",
    "    \"\"\" Custom accuracy cost function to be used in the scorer \"\"\";\n",
    "    # accuracy = correct predictions / total predictions\n",
    "\n",
    "    assert len(actual) == len(prediction);\n",
    "\n",
    "    return round((np.sum(actual == prediction) / len(actual)) , 4);\n",
    "\n",
    "\n",
    "class ColumnTypeFilter(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Custom transformer to select all columns of a particular type in a pandas dataframes \"\"\";\n",
    "\n",
    "    def __init__(self, dtype):\n",
    "        self.dtype = dtype;\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self;\n",
    "\n",
    "    def transform(self, X):\n",
    "        assert isinstance(X, pd.DataFrame);\n",
    "        return X.select_dtypes(include=[self.dtype]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(logger=None):\n",
    "    ''' Main routine to call the entire process flow ''';\n",
    "\n",
    "    # Load_Dataset --- Process starts\n",
    "\n",
    "    logger.info(f'');\n",
    "    logger.info(f'{\"-\"*20} Load dataset starts here {\"-\"*20}');\n",
    "    logger.info(f'');\n",
    "\n",
    "    # TODO: DONE; Load Cancer dataset;\n",
    "\n",
    "    cancer_data_dict = datasets.load_breast_cancer();\n",
    "    cancer_data_pd = convert2pandas_df(x_array=cancer_data_dict['data'],\n",
    "                      y=[ cancer_data_dict['target_names'][i] for i in cancer_data_dict['target'] ],\n",
    "                      # feature_names=iris_dict['feature_names'],\n",
    "                      feature_names=list(cancer_data_dict['feature_names']),\n",
    "                      target_name='Target');\n",
    "\n",
    "    # logger.info(f'{cancer_data_pd.head()}');\n",
    "\n",
    "    sns.lmplot( x=\"area error\", y=\"compactness error\", data=cancer_data_pd, fit_reg=False, hue='Target', legend=False,\n",
    "               palette=dict(malignant=\"#BF0C2B\", benign=\"#02173E\")); # , versicolor=\"#F5900E\"));\n",
    "    plt.legend(loc='lower right');\n",
    "    chart_save_image(plt=plt, f_size=(8, 8), left=0.125, right=0.9, bottom=0.125, top=0.9, wspace=0.0, hspace=0.0, fileName='./Cancer_Data_Plot.png');\n",
    "\n",
    "    selected_columns = ['Target', 'mean radius', 'mean texture', 'mean perimeter', 'mean area', 'mean smoothness', 'mean compactness', 'mean concavity',\n",
    "                        'mean concave points', 'mean symmetry'];\n",
    "\n",
    "    g = sns.pairplot(cancer_data_pd[selected_columns], hue=\"Target\", diag_kind=\"kde\",  palette=dict(malignant=\"#BF0C2B\", benign=\"#02173E\"), diag_kws=dict(shade=True));\n",
    "    for i, j in zip(*np.triu_indices_from(g.axes, 1)):\n",
    "        g.axes[i, j].set_visible(False);\n",
    "    chart_save_image(plt=plt, f_size=(16, 16), left=0.05, right=0.97, bottom=0.05, top=0.97, wspace=0.02, hspace=0.02, fileName='./Cancer_Data_PairPlot.png');\n",
    "\n",
    "    logger.info(f'');\n",
    "    logger.info(f'{\"-\"*20}  Load dataset ends here {\"-\"*20}');\n",
    "    logger.info(f'');\n",
    "\n",
    "    # Load_Dataset --- Process ends\n",
    "\n",
    "    # __Placeholder__ --- Process Starts\n",
    "\n",
    "    # TODO: DONE; 001; Train test split; stratified;\n",
    "    X_train, X_test, y_train, y_test = train_test_split(cancer_data_pd[cancer_data_dict.feature_names],\n",
    "                                                        # cancer_data_pd['Target'],\n",
    "                                                        cancer_data_dict['target'], # Has to be binary for scorer F1 and Percision;\n",
    "                                                        test_size=0.20,\n",
    "                                                        # stratify=cancer_data_pd['Target'],\n",
    "                                                        stratify=cancer_data_dict['target'],\n",
    "                                                        random_state=111,\n",
    "                                                        shuffle=True);\n",
    "\n",
    "    logger.info(f'{X_train.shape} {type(X_train)} {X_train.columns}');\n",
    "    logger.info(f'{X_test.shape}');\n",
    "    logger.info(f'{y_train.shape}');\n",
    "    logger.info(f'{y_test.shape}');\n",
    "\n",
    "    # TODO: DONE; 002; Dummy Classifier ;\n",
    "\n",
    "    # dummy_classifier = DummyClassifier(strategy=\"stratified\");\n",
    "    dummy_classifier = DummyClassifier(strategy=\"most_frequent\");\n",
    "\n",
    "    # TODO: DONE; 003; Cross_over_score and predict and Metrics (make_scorer)\n",
    "\n",
    "    accuracy_scorer = make_scorer(cost_accuracy, greater_is_better=True);\n",
    "\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=111);\n",
    "    # results = model_selection.cross_val_score(dummy_classifier, X_train, y_train, cv=kfold, scoring='accuracy');\n",
    "    # logger.info(f'{results} {np.mean(results)} {np.var(results)} {np.std(results)}');\n",
    "\n",
    "    results = model_selection.cross_val_score(dummy_classifier, X_train, y_train, cv=kfold, scoring=accuracy_scorer);\n",
    "    logger.info(f'{results} {np.mean(results)} {np.var(results)} {np.std(results)}');\n",
    "\n",
    "    DummyClassifier_mean = np.mean(results);\n",
    "\n",
    "    # TODO: DONE; 004; Standardization ;\n",
    "\n",
    "    # std_scaler = preprocessing.StandardScaler();  # Contains the negative values\n",
    "    std_scaler = preprocessing.MinMaxScaler(); # Range between 0 to 1; No negative terms;\n",
    "    std_scaler = std_scaler.fit(X_train);\n",
    "    scaled_X_train = pd.DataFrame(std_scaler.transform(X_train), columns=X_train.columns);\n",
    "\n",
    "    logger.info(f'{X_train[\"mean radius\"].describe()}');\n",
    "    logger.info(f'{scaled_X_train[\"mean radius\"].describe()}');\n",
    "\n",
    "    # TODO: DONE; 005; SelectKBest; Feature selection ;\n",
    "\n",
    "    # selectKbest_est = SelectKBest(chi2, k=4); f_classif\n",
    "    selectKbest_est = SelectKBest(f_classif, k=8);\n",
    "    selectKbest_X_train = selectKbest_est.fit_transform(X_train, y_train);\n",
    "\n",
    "    logger.info(f'{selectKbest_est.get_params(deep=True)}');\n",
    "    logger.info(f'{selectKbest_est.get_support(indices=False)}');\n",
    "    logger.info(f'{selectKbest_est.get_support(indices=True)}');\n",
    "    logger.info(f'{X_train.columns[selectKbest_est.get_support(indices=True)]}');\n",
    "\n",
    "    # TODO: DONE; 006; Polynomial Features ;\n",
    "\n",
    "    poly = preprocessing.PolynomialFeatures(degree=2, include_bias=False, interaction_only=False);\n",
    "    X_train_poly = poly.fit_transform(X_train);\n",
    "    X_train_p2 = pd.DataFrame(X_train_poly, columns=poly.get_feature_names(X_train.columns));\n",
    "\n",
    "    lr = linear_model.LogisticRegression(fit_intercept=False, random_state=111);\n",
    "    results = model_selection.cross_val_score(lr, X_train_p2, y_train, cv=kfold, scoring=accuracy_scorer); # , verbose=True);\n",
    "\n",
    "    imp_percentage = round((np.mean(results) - DummyClassifier_mean) / DummyClassifier_mean, 4);\n",
    "\n",
    "    logger.info(f'DummyClassifier accuracy : {DummyClassifier_mean}');\n",
    "    logger.info(f'LogisticRegression accuracy : {np.mean(results)}');\n",
    "\n",
    "    logger.info(f'The improvement over the DummyClassifier is : {imp_percentage}');\n",
    "\n",
    "    # TODO: DONE; 007; Kernel PCA ;\n",
    "\n",
    "    # kernel_param = ('rbf', 0.25);\n",
    "    kernel_param = ('rbf', 1);\n",
    "\n",
    "    kpca = KernelPCA(n_components=4, kernel=kernel_param[0], gamma=kernel_param[1], fit_inverse_transform=True, random_state=111) # n_jobs=-1,\n",
    "    kpca.fit(scaled_X_train);   # The data has to be scaled;\n",
    "    kpca_X_train = kpca.transform(scaled_X_train);\n",
    "\n",
    "    lr = linear_model.LogisticRegression(fit_intercept=False, random_state=111);\n",
    "    results = model_selection.cross_val_score(lr, kpca_X_train, y_train, cv=kfold, scoring=accuracy_scorer); # , verbose=True);\n",
    "\n",
    "    imp_percentage = round((np.mean(results) - DummyClassifier_mean) / DummyClassifier_mean, 4);\n",
    "\n",
    "    logger.info(f'DummyClassifier accuracy : {DummyClassifier_mean}');\n",
    "    logger.info(f'LogisticRegression accuracy : {np.mean(results)}');\n",
    "\n",
    "    logger.info(f'The improvement over the DummyClassifier is : {imp_percentage}');\n",
    "\n",
    "    # TODO: DONE; 008; Grid-Search ;\n",
    "\n",
    "    # tuned_parameters = [{\n",
    "    #                      'n_estimators' : [1, 10, 100, 500, 1000, 2000],\n",
    "    #                      'max_depth' : [10, 20],\n",
    "    #                      'max_features' : [0.80, 0.40],\n",
    "    #                      'random_state' : [111]\n",
    "    #                      }];\n",
    "\n",
    "    tuned_parameters = [{\n",
    "                         'n_estimators' : [1, 10],\n",
    "                         'max_depth' : [10, 20],\n",
    "                         'max_features' : [0.80, 0.40],\n",
    "                         'random_state' : [111]\n",
    "                         }];\n",
    "\n",
    "    clf = GridSearchCV(RandomForestClassifier(), tuned_parameters, cv=5, scoring=accuracy_scorer);\n",
    "    clf.fit(X_train, y_train);\n",
    "\n",
    "    logger.info(f'Best parameters set found on development set: {clf.best_score_} {clf.best_params_}');\n",
    "    logger.info('');\n",
    "    logger.info('Grid scores on development set:');\n",
    "    logger.info('');\n",
    "    means = clf.cv_results_['mean_test_score'];\n",
    "    stds = clf.cv_results_['std_test_score'];\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        logger.info(f'{round(mean,3)} (+/-{round(std*2,2)}) for {params}');\n",
    "    logger.info('');\n",
    "\n",
    "    logger.info('Detailed classification report:');\n",
    "    logger.info('');\n",
    "    logger.info('The model is trained on the full development set.');\n",
    "    logger.info('The scores are computed on the full evaluation set.');\n",
    "    logger.info('');\n",
    "    y_true, y_pred = y_test, clf.predict(X_test);\n",
    "    logger.info(f'{metrics.classification_report(y_true, y_pred)}');\n",
    "    logger.info('');\n",
    "\n",
    "    imp_percentage = round((clf.best_score_ - DummyClassifier_mean) / DummyClassifier_mean, 4);\n",
    "    logger.info(f'DummyClassifier accuracy : {DummyClassifier_mean}');\n",
    "    logger.info(f'GridSearchCV RandomForestClassifier accuracy : {clf.best_score_}');\n",
    "    logger.info(f'The improvement over the DummyClassifier is : {imp_percentage}');\n",
    "\n",
    "    # logger.info(f'{clf.best_estimator_}');\n",
    "\n",
    "    # TODO: DONE; 009; Customer Transformer for the pipeline ;\n",
    "    # reference : https://ramhiser.com/post/2018-04-16-building-scikit-learn-pipeline-with-pandas-dataframe/\n",
    "    # http://philipmgoddard.com/modeling/sklearn_pipelines\n",
    "\n",
    "    ctf = ColumnTypeFilter(np.number);\n",
    "    ctf.fit_transform(X_train).head();\n",
    "\n",
    "    # TODO: YTS; 010; Pipeline ;\n",
    "\n",
    "    custom_pipeline = make_pipeline(\n",
    "            FeatureUnion(transformer_list=[\n",
    "                ('StdScl', make_pipeline(\n",
    "                    ColumnTypeFilter(np.number),\n",
    "                    preprocessing.StandardScaler()\n",
    "                )),\n",
    "                ('MMScl', make_pipeline(\n",
    "                    ColumnTypeFilter(np.number),\n",
    "                    preprocessing.MinMaxScaler()\n",
    "                ))\n",
    "            ])\n",
    "    );\n",
    "\n",
    "    custom_pipeline.fit(X_train);\n",
    "    X_test_transformed = custom_pipeline.transform(X_test);\n",
    "\n",
    "    logger.info(f'{X_test.shape} {type(X_test_transformed)} {X_test_transformed.shape}');\n",
    "\n",
    "    # TODO: DONE; 011; Ensemble (VotingClassifier) and BaseClone;\n",
    "\n",
    "    ensemble_clf = VotingClassifier(estimators=[\n",
    "                            ('dummy', dummy_classifier),\n",
    "                            ('logistic', lr),\n",
    "                            # ('supportvector', SVC(probability=True)),\n",
    "                            ('randomforest', RandomForestClassifier())],\n",
    "                            voting='soft');\n",
    "\n",
    "    ensemble_clf.fit(X_train, y_train);\n",
    "    ensemble_clf_accuracy_ = cost_accuracy(y_test, ensemble_clf.predict(X_test));\n",
    "\n",
    "    imp_percentage = round((ensemble_clf_accuracy_ - DummyClassifier_mean) / DummyClassifier_mean, 4);\n",
    "    logger.info(f'DummyClassifier accuracy : {DummyClassifier_mean}');\n",
    "    logger.info(f'GridSearchCV RandomForestClassifier accuracy : {ensemble_clf_accuracy_}');\n",
    "    logger.info(f'The improvement over the DummyClassifier is : {imp_percentage}');\n",
    "\n",
    "    # TODO: DONE; 012; One-hot encoder; Label Encoder; Binary Encoder;\n",
    "\n",
    "    baby_names = ['Ava', 'Lily', 'Noah', 'Jacob', 'Mia', 'Sophia'];\n",
    "    X_train_list = [ np.random.choice(baby_names) for i in range(40) ];\n",
    "    X_test_list = [ np.random.choice(baby_names) for i in range(6) ];\n",
    "\n",
    "    bb_labelencoder = preprocessing.LabelEncoder();\n",
    "    bb_labelencoder.fit(X_train_list);\n",
    "    bb_encoded = bb_labelencoder.transform(X_test_list);\n",
    "\n",
    "    bb_onehotencoder = preprocessing.OneHotEncoder(sparse=False);\n",
    "    bb_encoded = bb_encoded.reshape(len(bb_encoded), 1);\n",
    "    bb_onehot = bb_onehotencoder.fit_transform(bb_encoded);\n",
    "\n",
    "    for i, v in enumerate(X_test_list):\n",
    "        logger.info(f'Actual : {v} \\t | LabelEncoded : {bb_encoded[i][0]} \\t | OneHot : {bb_onehot[i]}');\n",
    "\n",
    "    # TODO: DONE; 013; Feature Extraction from image and text;\n",
    "\n",
    "    corpus = [  'This is the first document.',\n",
    "                'This document is the second document.',\n",
    "                'And this is the third one.',\n",
    "                'Is this the first document?', ]\n",
    "\n",
    "    vectorizer = CountVectorizer();\n",
    "    X = vectorizer.fit_transform(corpus);\n",
    "\n",
    "    cntvector_out = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names());\n",
    "\n",
    "    for i, v in enumerate(corpus):\n",
    "        logger.info(f'Input text : {v}');\n",
    "        logger.info(f'Output counter vector : {v}');\n",
    "        logger.info(f'{cntvector_out.iloc[i]}');\n",
    "\n",
    "    # TODO: DONE; 014; Utils for dumping and loading models;\n",
    "\n",
    "    # Util shuffle;\n",
    "    # Util Resample;\n",
    "\n",
    "    # __Placeholder__ --- Process ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3181fb577f14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLOG_LEVEL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mformatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFormatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%(asctime)s - %(levelname)s - %(message)s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%d/%m %H:%M:%S'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'{\".\".join(__file__.split(\".\")[:-1])}_{LOG_TS}.log'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mfh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLOG_LEVEL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mfh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetFormatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    LOG_TS = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S');\n",
    "    LOG_LEVEL = logging.DEBUG;\n",
    "    # (LogLevel : Numeric_value) : (CRITICAL : 50) (ERROR : 40) (WARNING : 30) (INFO : 20) (DEBUG : 10) (NOTSET : 0)\n",
    "\n",
    "    DELETE_FLAG = True;\n",
    "    extension_list = ['.log'];  # File extensions to delete after the run;\n",
    "    ts = time();\n",
    "\n",
    "    logger = logging.getLogger(__name__);\n",
    "    logger.setLevel(LOG_LEVEL);\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', '%d/%m %H:%M:%S');\n",
    "    fh = logging.FileHandler(filename=f'{\".\".join(__file__.split(\".\")[:-1])}_{LOG_TS}.log');\n",
    "    fh.setLevel(LOG_LEVEL);\n",
    "    fh.setFormatter(formatter);\n",
    "    logger.addHandler(fh);\n",
    "\n",
    "    Test_case = f'Scikit Learn Machine Learning Workflow Process Code : {LOG_TS}';\n",
    "    Test_comment = '-' * len(Test_case);\n",
    "\n",
    "    logger.info(Test_comment);\n",
    "    logger.info(Test_case);\n",
    "    logger.info(Test_comment);\n",
    "\n",
    "    delete_old_log_files(delete_flag=DELETE_FLAG, logger=logger, extension_list=extension_list);\n",
    "    main(logger=logger);\n",
    "\n",
    "    logger.info(Test_comment);\n",
    "    logger.info(f'Code execution took {round((time() - ts), 4)} seconds');\n",
    "    logger.info(Test_comment);\n",
    "\n",
    "\n",
    "# Temp --- Process starts\n",
    "# Testing and unused code blocks for reference;\n",
    "# Temp --- Process ends\n",
    "#\n",
    "# iris_dict = datasets.load_iris();\n",
    "# iris_pd = convert2pandas_df(x_array=iris_dict['data'],\n",
    "#                   y=[ iris_dict['target_names'][i] for i in iris_dict['target'] ],\n",
    "#                   # feature_names=iris_dict['feature_names'],\n",
    "#                   feature_names=['petal_length', 'petal_width', 'sepal_length', 'sepal_width'],\n",
    "#                   target_name='Flower_Class');\n",
    "#\n",
    "# logger.info(f'{iris_pd.head()}');\n",
    "#\n",
    "# moons_tuple = datasets.make_moons(n_samples=100);\n",
    "# moons_pd = convert2pandas_df(x_array=moons_tuple[0],\n",
    "#                   y=list(moons_tuple[1]),\n",
    "#                   feature_names=['x_cord', 'y_cord'],\n",
    "#                   target_name='category');\n",
    "#\n",
    "# logger.info(f'{moons_pd.head()}');\n",
    "#\n",
    "# # sns.lmplot( x=\"x_cord\", y=\"y_cord\", data=moons_pd, fit_reg=False, hue='category', legend=False);\n",
    "# sns.lmplot( x=\"petal_length\", y=\"petal_width\", data=iris_pd, fit_reg=False, hue='Flower_Class', legend=False,\n",
    "#            # palette=dict(setosa=\"#9b59b6\", virginica=\"#3498db\", versicolor=\"#95a5a6\"));\n",
    "#            # palette=dict(setosa=\"#BF0C2B\", virginica=\"#02173E\", versicolor=\"#F14C13\"));\n",
    "#            palette=dict(setosa=\"#BF0C2B\", virginica=\"#02173E\", versicolor=\"#F5900E\"));\n",
    "#            # Kopie von Copy of コピー rocket021x; Adobe color palette;\n",
    "#\n",
    "# plt.legend(loc='lower right');\n",
    "#\n",
    "# chart_save_image(plt=plt, f_size=(8, 8), left=0.125, right=0.9, bottom=0.125, top=0.9, wspace=0.0, hspace=0.0, fileName='./Iris_Plot.png');\n",
    "#\n",
    "# tips = sns.load_dataset(\"tips\");\n",
    "# sns.violinplot(x = \"total_bill\", data=tips);\n",
    "# chart_save_image(plt=plt, f_size=None, left=0.125, right=0.9, bottom=0.125, top=0.9, wspace=0.0, hspace=0.0, fileName='./Violin_Plot.png');\n",
    "#\n",
    "# blobs_tuple = datasets.make_blobs(n_samples=500);\n",
    "# blobs_pd = convert2pandas_df(x_array=blobs_tuple[0],\n",
    "#                   y=list(blobs_tuple[1]),\n",
    "#                   feature_names=['x_cord', 'y_cord'],\n",
    "#                   target_name='category');\n",
    "#\n",
    "# sns.lmplot( x=\"x_cord\", y=\"y_cord\", data=blobs_pd, fit_reg=False, hue='category', legend=False);\n",
    "# plt.legend(loc='lower right');\n",
    "# chart_save_image(plt=plt, f_size=None, left=0.125, right=0.9, bottom=0.125, top=0.9, wspace=0.0, hspace=0.0, fileName='./Blob_Plot.png');\n",
    "#\n",
    "# g = sns.pairplot(iris_pd, hue=\"Flower_Class\", diag_kind=\"kde\", palette=dict(setosa=\"#BF0C2B\", virginica=\"#02173E\", versicolor=\"#F5900E\"), diag_kws=dict(shade=True));\n",
    "# for i, j in zip(*np.triu_indices_from(g.axes, 1)):\n",
    "#     g.axes[i, j].set_visible(False)\n",
    "# chart_save_image(plt=plt, f_size=(16, 16), left=0.05, right=0.97, bottom=0.05, top=0.97, wspace=0.05, hspace=0.06, fileName='./Iris_PairPlot.png');\n",
    "# # Left and Right are in pixel percentage positions;\n",
    "#\n",
    "# # JointPlot with HEX Bins\n",
    "# # with sns.axes_style('white'):\n",
    "# #     sns.jointplot(\"x_cord\", \"y_cord\", blobs_pd, kind='hex');\n",
    "#\n",
    "# sns.distplot(iris_pd['petal_length']);\n",
    "# chart_save_image(plt=plt, left=0.05, right=0.97, bottom=0.05, top=0.97, wspace=0.05, hspace=0.06, fileName='./Joint_Plot.png');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
